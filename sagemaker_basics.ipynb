{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1121aeca",
   "metadata": {},
   "source": [
    "# ðŸš€ SageMaker Basics: Interacting with S3 and Running a Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d0764f",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed669e7",
   "metadata": {},
   "source": [
    "## 2. Setting up SageMaker Session and Execution Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe68440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get the execution role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Get the region\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e27ab3",
   "metadata": {},
   "source": [
    "## 3. Basic S3 Interactions\n",
    "\n",
    "> **Tip:** You can interact with S3 using **boto3** or **SageMaker utilities**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b812c",
   "metadata": {},
   "source": [
    "### 3.1 List all buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee351f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# List all buckets\n",
    "buckets = [bucket['Name'] for bucket in s3_client.list_buckets()['Buckets']]\n",
    "print(\"S3 Buckets:\", buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2813f365",
   "metadata": {},
   "source": [
    "### 3.2 Create a new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique bucket name (bucket names must be globally unique)\n",
    "bucket_name = f\"sagemaker-example-{uuid.uuid4()}\"\n",
    "s3_client.create_bucket(Bucket=bucket_name)\n",
    "print(f\"Created bucket: {bucket_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2abab4",
   "metadata": {},
   "source": [
    "### 3.3 Upload a file to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple file\n",
    "filename = 'example.txt'\n",
    "with open(filename, 'w') as f:\n",
    "    f.write('This is a sample file to upload to S3.')\n",
    "\n",
    "# Upload the file\n",
    "s3_client.upload_file(filename, bucket_name, filename)\n",
    "print(f\"Uploaded {filename} to bucket {bucket_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f8a69",
   "metadata": {},
   "source": [
    "## 4. Running a Basic Training Job on SageMaker\n",
    "\n",
    "> **We will use a built-in SageMaker algorithm** (e.g., XGBoost) for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b63fc",
   "metadata": {},
   "source": [
    "### 4.1 Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20dd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 URI for training data (provided by AWS)\n",
    "training_data_uri = 's3://sagemaker-sample-data-{}/tensorflow/mnist'.format(region) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4205f6",
   "metadata": {},
   "source": [
    "### 4.2 Construct a script for distributed training\n",
    "This tutorial's training script was adapted from TensorFlow's official [CNN MNIST example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py). We have modified it to handle the `model_dir` parameter passed in by SageMaker. This is an S3 path which can be used for data sharing during distributed training and checkpointing and/or model persistence. We have also added an argument-parsing function to handle processing training-related variables.\n",
    "\n",
    "At the end of the training job we have added a step to export the trained model to the path stored in the environment variable `SM_MODEL_DIR`, which always points to `/opt/ml/model`. This is critical because SageMaker uploads all the model artifacts in this folder to S3 at end of training.\n",
    "\n",
    "Here is the entire script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 2.1 script\n",
    "!pygmentize 'mnist-2.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb59a4",
   "metadata": {},
   "source": [
    "### 4.3 Create a training job using the TensorFlow estimator\n",
    "\n",
    "The sagemaker.tensorflow.TensorFlow estimator handles locating the script mode container, uploading your script to a S3 location and creating a SageMaker training job. Let's call out a couple important parameters here:\n",
    "\n",
    "py_version is set to 'py3' to indicate that we are using script mode since legacy mode supports only Python 2. We do not reccommend to run Tensorflow in legacy mode with Python 2.\n",
    "\n",
    "distribution is used to configure the distributed training setup. It's required only if you are doing distributed training either across a cluster of instances or across multiple GPUs. Here we are using parameter servers as the distributed training schema. SageMaker training jobs run on homogeneous clusters. To make parameter server more performant in the SageMaker setup, we run a parameter server on every instance in the cluster, so there is no need to specify the number of parameter servers to launch. Script mode also supports distributed training with Horovod. You can find the full documentation on how to configure distributions here.\n",
    "\n",
    "instance_type specify the EC2 instance used for training. You should right-size your training instance based on the size of your data, algorithm and tasks. Here we choose ml.c5.xlarge. You can also read more about G4dn instances, which feature NVIDIA T4 GPUs and custom Intel Cascade Lake CPUs, and are optimized for machine learning inference and small scale training. Read more on available instance types and pricing.\n",
    "\n",
    "use_spot_instances(Optional): For further cost optimization, you can leverage managed Amazon EC2 Spot instances by setting this parameter to True. Managed spot training can optimize the cost of training models up to 90% over on-demand instances. SageMaker manages the Spot interruptions on your behalf. You can specify which training jobs use spot instances and a stopping condition that specifies how long Amazon SageMaker waits for a job to run using Amazon EC2 Spot instances. Full documentation here.\n",
    "\n",
    "You can initialize an estimator to train with TensorFlow 2.1 script and you will need to specify the right framework_version, i.e., 2.1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(\n",
    "    entry_point='mnist-2.py',\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    framework_version='2.1.0',\n",
    "    py_version='py3',\n",
    "    distribution={'parameter_server': {'enabled': True}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efc4c7",
   "metadata": {},
   "source": [
    "### 4.4 Calling fit\n",
    "To start a training job, we call estimator.fit(training_data_uri).\n",
    "\n",
    "An S3 location is used here as the input. fit creates a default channel named 'training', which points to this S3 location. In the training script we can then access the training data from the location stored in SM_CHANNEL_TRAINING. fit accepts a couple other types of input as well. See the API doc here for details.\n",
    "\n",
    "When training starts, the TensorFlow container executes mnist.py, passing hyperparameters and model_dir from the estimator as script arguments. Because we didn't define either in this example, no hyperparameters are passed, and model_dir defaults to s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>, so the script execution is as follows:\n",
    "\n",
    "python mnist-2.py --model_dir s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>\n",
    "\n",
    "When training is complete, the training job will upload the saved model for TensorFlow serving.\n",
    "\n",
    "Calling fit to train a model with TensorFlow 2.1 script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9924c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_estimator.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1928d7",
   "metadata": {},
   "source": [
    "## 5. Deploying an endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28215fa",
   "metadata": {},
   "source": [
    "### 5.1 Deploy the trained model to an endpoint\n",
    "The deploy() method creates a SageMaker model, which is then deployed to an endpoint to serve prediction requests in real time. We will use the TensorFlow Serving container for the endpoint, because we trained with script mode. This serving container runs an implementation of a web server that is compatible with SageMaker hosting protocol. The Using your own inference code document explains how SageMaker runs inference containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mnist_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd42495",
   "metadata": {},
   "source": [
    "### 5.2 Invoke the endpoint\n",
    "Let's download the training data and use that as input for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af452c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "!aws --region {region} s3 cp s3://sagemaker-sample-data-{region}/tensorflow/mnist/train_data.npy train_data.npy\n",
    "!aws --region {region} s3 cp s3://sagemaker-sample-data-{region}/tensorflow/mnist/train_labels.npy train_labels.npy\n",
    "\n",
    "train_data = np.load('train_data.npy')\n",
    "train_labels = np.load('train_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d179259",
   "metadata": {},
   "source": [
    "The formats of the input and the output data correspond directly to the request and response formats of the Predict method in the TensorFlow Serving REST API. SageMaker's TensforFlow Serving endpoints can also accept additional input formats that are not part of the TensorFlow REST API, including the simplified JSON format, line-delimited JSON objects (\"jsons\" or \"jsonlines\"), and CSV data.\n",
    "\n",
    "In this example we are using a numpy array as input, which will be serialized into the simplified JSON format. In addtion, TensorFlow serving can also process multiple items at once as you can see in the following code. You can find the complete documentation on how to make predictions against a TensorFlow serving SageMaker endpoint here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 08\n",
    "predictions = predictor.predict(train_data[:50])\n",
    "for i in range(0, 50):\n",
    "    prediction = np.argmax(predictions['predictions'][i])\n",
    "    label = train_labels[i]\n",
    "    print('prediction is {}, label is {}, matched: {}'.format(prediction, label, prediction == label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3767f0bc",
   "metadata": {},
   "source": [
    "# Delete the endpoint\n",
    "Let's delete the endpoint we just created to prevent incurring any extra costs and then [verify](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34008e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
